{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objectives__\n",
    "\n",
    "- What is polynomial regression? Why do we need them?\n",
    "\n",
    "- Use sklearn to generate polynomial features.\n",
    "\n",
    "- What is training error and test error?\n",
    "\n",
    "- What is Bias and Variance in ML? \n",
    "\n",
    "- What is underfitting and overfitting?\n",
    "\n",
    "# Polynomial Regression\n",
    "\n",
    "Recall that before we discussed the linear models. Today we will see that the linearity assumption is very insufficient to explain complex relations. In order to be able to address this problem we will see how to add higher orders of a variable to detect non-linear relations between X and y. \n",
    "\n",
    "- In polynomial regression our model will look like:\n",
    "\n",
    "$$\\hat y_i = \\hat\\beta_0+\\hat\\beta_1x_i +\\hat\\beta_2 x^2_i+\\hat\\beta_3 x^3_i+...+\\hat\\beta_d x^d_i+\\epsilon_i$$ \n",
    "\n",
    "- For the loss function we will be still using RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uniformly sample from -2pi and 2pi. We will be working with 150 observations.\n",
    "\n",
    "x = np.random.uniform(-2*pi, 2*pi, 150)\n",
    "\n",
    "## We created the target variables y: So we know the true relation between x and y \n",
    "## Note also that we added some noise to the y variable.\n",
    "\n",
    "y = np.sin(x) + np.random.normal(loc = 0, scale = 0.4, size = len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's plot the data\n",
    "plt.scatter(x,y)\n",
    "\n",
    "plt.ylabel('$\\sin(x)$')\n",
    "\n",
    "plt.xlabel('x values are randomly chosen from $[-2\\pi, 2\\pi]$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn lmplot to visualize regression line \n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(data = {'x': x, 'y':y})\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.lmplot(x = 'x', y= 'y', data = data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is clear from the plot the linear model is insufficient to capture the pattern in the data. This is called __underfitting__. One solution to this problem is to increase the number of variables by adding higher orders of the already existing variables. \n",
    "\n",
    "__Your Turn__\n",
    "\n",
    "- We will use PolynomialFeature class to be able to create new features.\n",
    "\n",
    "- Create a new dataset with all the powers of x up to 7 degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's use PolynomialFeature to create higher order features\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-5 supplement.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a linear model by either using sklearn.linearmodel or statsmodel.api \n",
    "\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Use the dataset you created above and fit a linear model to this dataset.\n",
    "\n",
    "- Make predictions using this data and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load -r 8-14 supplement.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Now plot both predictions and true values and analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ It looks like polynomial variables works very well, then why don't we add polynomial features all the time?\n",
    "\n",
    "# Bias - Variance\n",
    "\n",
    "In statistical learning and machine learning we start with a model:\n",
    "\n",
    "$$ Y = f(X) + \\epsilon$$\n",
    "\n",
    "Here $f$ is the function that gives the 'true' relationship between $X$ values and $Y$ values.\n",
    "\n",
    "- In many situation, from a given a train set $X, y$ we would like to find an estimate $\\hat{f}$ of $f$. \n",
    "\n",
    "- Notice that the actual $f$ might be very complicated and $\\hat{f}$ will be a simplified estimator of $f$ in general.\n",
    "\n",
    "- We can evaluate the models performance with a metric. In regression setting it is mostly given as MSE:\n",
    "\n",
    "\n",
    "$$ MSE = \\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_{i} - \\hat{f}(x_{i}))^{2}$$\n",
    "\n",
    "- However, we are really not interested in whether $\\hat{f}(x_i) \\sim y_i$; instead, we want to know whether $\\hat{f}(x_0)$ is approximately equal to $y_{0}$, where $(x_0, y_0)$ is a previously unseen test observation not used to train the statistical learning method.\n",
    "\n",
    "<img src=\"img/test_train.png\" cap=\"Transformed dataset\"  width='600'/>\n",
    "\n",
    "- This U-shape we observe on the right between test error and training error is a very common and result of two competing notion: 'Bias' and 'Variance' of the model. More excplicitly we say that the 'average' test error can be decompose into three pieces.\n",
    "\n",
    "<img src=\"img/bias_variance.png\" cap=\"Transformed dataset\"  width='400'/>\n",
    "\n",
    "__A good model__\n",
    "\n",
    "- Low error --> Low variance + low bias\n",
    "\n",
    "\n",
    "__Variance__\n",
    "\n",
    "- Refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set.\n",
    "\n",
    "__Bias__\n",
    "\n",
    "- Refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.\n",
    "\n",
    "\n",
    "__Q:__ Find a model with a very high bias and very low variance.\n",
    "\n",
    "__Q:__ Find a model with a very high variance and very low bias.\n",
    "\n",
    "\n",
    "__Your Turn__\n",
    "\n",
    "I created a data set and stored this as a pickle. When you load the pickle we will have a list of the form [x,y]. We know that there is a polynomial relationship between x and y. Now the question is to find this relation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r\"data_list_xy.pickle\", \"rb\") as input_file:\n",
    "        data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[0]\n",
    "\n",
    "y = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "plt.scatter(x,y)\n",
    "\n",
    "plt.xlabel(\"x's are randomly sampled from [-1,1]\" )\n",
    "\n",
    "plt.ylabel(\"$y = \\\\beta_{0} + \\\\beta_{1}x + \\\\cdots + \\\\beta_{p}x^{p}$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- We know that the true relation between x and y above is a polynomial one. \n",
    "\n",
    "- Try to find this relationship\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 25-32 supplement.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function returns the dataset of a given polynomial degree\n",
    "def create_dataset(x, degree):\n",
    "    ## Instantiate the PolynomialFeatures object with given 'degree'\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    ## Now transform data to create higher order features\n",
    "    new_data = poly.fit_transform(x.reshape(-1, 1))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Documentation__\n",
    "\n",
    "[Sklearn-PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "\n",
    "\n",
    "__Fit a linear model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coefficients(data, y):\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    \n",
    "    lr.fit(data, y)\n",
    "    \n",
    "    print(lr.coef_)\n",
    "    \n",
    "    y_pred = lr.predict(data)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the dataset with given degree\n",
    "new_x = create_dataset(x, degree= 25)\n",
    "\n",
    "## fit a linear regression and print coefficients return predictions\n",
    "predict = find_coefficients(new_x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot predictions and true values\n",
    "## discuss the results.\n",
    "def plot_predict(x,y,y_pred):\n",
    "    plt.scatter(x,y, label = 'true')\n",
    "    plt.scatter(x, y_pred, label = 'train')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to find out the true parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 35-37 supplement.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart below is very common online so I wanted to share here. I think this gives a nice summary but please note that this is absolutely representative.\n",
    "\n",
    "<img src=\"https://hsto.org/files/281/108/1e9/2811081e9eda44d08f350be5a9deb564.png\" width=350, height=350>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
